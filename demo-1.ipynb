{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# def clean_forex_data(filepath):\n",
    "#     \"\"\"Load and clean forex data for machine learning.\"\"\"\n",
    "#     # Load the data\n",
    "#     data = pd.read_csv(\n",
    "#         filepath, sep=\"\\t\", dtype=str\n",
    "#     )  # Use sep='\\t' for tab-separated values\n",
    "\n",
    "#     # Combine <DATE> and <TIME> into a single datetime column\n",
    "#     data[\"datetime\"] = pd.to_datetime(data[\"<DATE>\"] + \" \" + data[\"<TIME>\"])\n",
    "\n",
    "#     # Drop the original <DATE> and <TIME> columns\n",
    "#     data.drop(columns=[\"<DATE>\", \"<TIME>\"], inplace=True)\n",
    "\n",
    "#     # Rename columns to standard names\n",
    "#     data.rename(\n",
    "#         columns={\n",
    "#             \"<OPEN>\": \"open\",\n",
    "#             \"<HIGH>\": \"high\",\n",
    "#             \"<LOW>\": \"low\",\n",
    "#             \"<CLOSE>\": \"close\",\n",
    "#             \"<TICKVOL>\": \"tick_volume\",\n",
    "#             \"<VOL>\": \"volume\",\n",
    "#             \"<SPREAD>\": \"spread\",\n",
    "#         },\n",
    "#         inplace=True,\n",
    "#     )\n",
    "\n",
    "#     # Convert numerical columns to appropriate types\n",
    "#     numerical_cols = [\"open\", \"high\", \"low\", \"close\", \"tick_volume\", \"volume\", \"spread\"]\n",
    "#     data[numerical_cols] = data[numerical_cols].astype(float)\n",
    "#     # Set the datetime column as the index\n",
    "#     data.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "#     # Reset index if needed\n",
    "#     # data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# cleaned_data = clean_forex_data(\"GBPUSD_M15.csv\")\n",
    "\n",
    "# # Display the first few rows of the cleaned data\n",
    "# cleaned_data.head(2)\n",
    "\n",
    "# cleaned_data.to_csv(\"cleaned_GBPUSD_M15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MetaTrader5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMetaTrader5\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmt5\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MetaTrader5'"
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt5\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tf2onnx\n",
    "import keras\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM\n",
    "from keras.metrics import RootMeanSquaredError as rmse\n",
    "from tensorflow.keras import callbacks\n",
    "from sys import argv\n",
    "\n",
    "# Constants\n",
    "inp_history_size = 120\n",
    "sample_size = inp_history_size * 3 * 20\n",
    "symbol = \"EURUSD\"\n",
    "optional = \"D1_2024\"\n",
    "inp_model_name = f\"{symbol}_{optional}.onnx\"\n",
    "\n",
    "# Initialize MetaTrader 5\n",
    "if not mt5.initialize():\n",
    "    print(\"initialize() failed, error code =\", mt5.last_error())\n",
    "    quit()\n",
    "\n",
    "# Data paths\n",
    "data_path = argv[0]\n",
    "last_index = data_path.rfind(\"\\\\\") + 1\n",
    "data_path = data_path[0:last_index]\n",
    "print(\"Data path to save ONNX model:\", data_path)\n",
    "\n",
    "# File path for saving\n",
    "terminal_info = mt5.terminal_info()\n",
    "file_path = terminal_info.data_path + \"\\\\MQL5\\\\Files\\\\\"\n",
    "print(\"File path to save ONNX model:\", file_path)\n",
    "\n",
    "# Set start and end dates for history data\n",
    "end_date = datetime(2024, 1, 1, 0)\n",
    "start_date = end_date - timedelta(days=inp_history_size * 20 * 3)\n",
    "print(\"Data start date =\", start_date)\n",
    "print(\"Data end date =\", end_date)\n",
    "\n",
    "# Get rates\n",
    "eurusd_rates = mt5.copy_rates_from(symbol, mt5.TIMEFRAME_D1, end_date, sample_size)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(\n",
    "    eurusd_rates\n",
    ")  # Assume 'eurusd_rates' is the correct data\n",
    "\n",
    "# Split data into training and testing\n",
    "training_size = int(len(scaled_data) * 0.80)\n",
    "print(\"Training size:\", training_size)\n",
    "train_data_initial = scaled_data[:training_size, :]\n",
    "test_data_initial = scaled_data[training_size:, :1]\n",
    "\n",
    "\n",
    "# Function to split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Split into samples\n",
    "time_step = inp_history_size\n",
    "x_train, y_train = split_sequence(train_data_initial, time_step)\n",
    "x_test, y_test = split_sequence(test_data_initial, time_step)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Conv1D(\n",
    "        filters=256,\n",
    "        kernel_size=2,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(inp_history_size, 1),\n",
    "    )\n",
    ")\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse()])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=300,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Evaluate training data\n",
    "train_loss, train_rmse = model.evaluate(x_train, y_train, batch_size=32)\n",
    "print(f\"Train loss={train_loss:.3f}\")\n",
    "print(f\"Train RMSE={train_rmse:.3f}\")\n",
    "\n",
    "# Evaluate testing data\n",
    "test_loss, test_rmse = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(f\"Test loss={test_loss:.3f}\")\n",
    "print(f\"Test RMSE={test_rmse:.3f}\")\n",
    "\n",
    "\n",
    "# Define a function to represent the model\n",
    "@tf.function(input_signature=[tf.TensorSpec([None, inp_history_size, 1], tf.float32)])\n",
    "def model_function(x):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "# Convert the model to ONNX\n",
    "output_path = data_path + inp_model_name\n",
    "onnx_model, _ = tf2onnx.convert.from_function(\n",
    "    model_function,\n",
    "    input_signature=[tf.TensorSpec([None, inp_history_size, 1], tf.float32)],\n",
    "    opset=13,\n",
    "    output_path=output_path,\n",
    ")\n",
    "\n",
    "print(f\"Saved ONNX model to {output_path}\")\n",
    "\n",
    "# Save model to ONNX in both paths\n",
    "output_path = file_path + inp_model_name\n",
    "onnx_model = tf2onnx.convert.from_keras(model, output_path=output_path)\n",
    "print(f\"Saved model to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-env-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
